var tree =
[
  {
    "text": "Math",
    "item-id": "c12,iNone",
    "nodes": [
      {
        "text": "Stats",
        "item-id": "c10,i21",
        "nodes": [
          {
            "text": "Weighted kappa",
            "item-id": "i21",
            "nodes": [
              {
                "text": "cohens-kappa.pdf",
                "item-id": "i20",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "cohens-kappa.pdf",
                "item_type": "attachment",
                "metadata": [
                  [
                    "title",
                    "cohens-kappa.pdf"
                  ]
                ],
                "resource": "storage/KZ33KNBM/cohens-kappa.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Weighted kappa",
            "item_type": "journalArticle",
            "metadata": [
              [
                "volume",
                "70"
              ],
              [
                "issue",
                "4"
              ],
              [
                "pages",
                "213"
              ],
              [
                "publicationTitle",
                "Psychological bulletin"
              ],
              [
                "date",
                "1968-00-00 1968"
              ],
              [
                "libraryCatalog",
                "Google Scholar"
              ],
              [
                "title",
                "Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit."
              ],
              [
                "shortTitle",
                "Weighted kappa"
              ]
            ],
            "selectable": false
          }
        ],
        "item_title": "Stats",
        "item_type": null,
        "selectable": false
      }
    ],
    "item_title": "Math",
    "item_type": null,
    "selectable": false
  },
  {
    "text": "NLP",
    "item-id": "c5,i383",
    "nodes": [
      {
        "text": "Word Vectors",
        "item-id": "c6,i283",
        "nodes": [
          {
            "text": "ConceptNet at SemEval-2017 Task 2",
            "item-id": "i279",
            "nodes": [
              {
                "text": "Comment: 5 pages, accepted to the SemEval workshop at ACL 2017",
                "item-id": "n280",
                "icon": "glyphicon glyphicon-text-background",
                "item_title": "Comment: 5 pages, accepted to the SemEval workshop at ACL 2017",
                "item_type": "note"
              },
              {
                "text": "arXiv:1704.03560 PDF",
                "item-id": "i281",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "arXiv:1704.03560 PDF",
                "item_type": "attachment",
                "metadata": [
                  [
                    "url",
                    "http://www.arxiv.org/pdf/1704.03560.pdf"
                  ],
                  [
                    "accessDate",
                    "2018-04-09 15:50:47"
                  ],
                  [
                    "title",
                    "arXiv:1704.03560 PDF"
                  ]
                ],
                "resource": "storage/JXX38RLR/Speer_and_Lowry-Duda_-_2017_-_ConceptNet_at_SemEval-2017_Task_2_Extending_Word_.pdf"
              },
              {
                "text": "arXiv.org Snapshot",
                "item-id": "i282",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "arXiv.org Snapshot",
                "item_type": "attachment",
                "metadata": [
                  [
                    "url",
                    "https://arxiv.org/abs/1704.03560"
                  ],
                  [
                    "accessDate",
                    "2018-04-09 15:50:48"
                  ],
                  [
                    "title",
                    "arXiv.org Snapshot"
                  ]
                ],
                "resource": "storage/VXZGYS9Z/1704.html"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "ConceptNet at SemEval-2017 Task 2",
            "item_type": "journalArticle",
            "metadata": [
              [
                "url",
                "http://arxiv.org/abs/1704.03560"
              ],
              [
                "publicationTitle",
                "arXiv:1704.03560 [cs]"
              ],
              [
                "date",
                "2017-04-11 2017-04-11"
              ],
              [
                "extra",
                "arXiv: 1704.03560"
              ],
              [
                "accessDate",
                "2018-04-09 15:50:46"
              ],
              [
                "libraryCatalog",
                "arXiv.org"
              ],
              [
                "abstractNote",
                "This paper describes Luminoso's participation in SemEval 2017 Task 2, \"Multilingual and Cross-lingual Semantic Word Similarity\", with a system based on ConceptNet. ConceptNet is an open, multilingual knowledge graph that focuses on general knowledge that relates the meanings of words and phrases. Our submission to SemEval was an update of previous work that builds high-quality, multilingual word embeddings from a combination of ConceptNet and distributional semantics. Our system took first place in both subtasks. It ranked first in 4 out of 5 of the separate languages, and also ranked first in all 10 of the cross-lingual language pairs."
              ],
              [
                "title",
                "ConceptNet at SemEval-2017 Task 2: Extending Word Embeddings with Multilingual Relational Knowledge"
              ],
              [
                "shortTitle",
                "ConceptNet at SemEval-2017 Task 2"
              ]
            ],
            "selectable": false
          },
          {
            "text": "ConceptNet 5.5",
            "item-id": "i283",
            "nodes": [
              {
                "text": "arXiv:1612.03975 PDF",
                "item-id": "i284",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "arXiv:1612.03975 PDF",
                "item_type": "attachment",
                "metadata": [
                  [
                    "url",
                    "http://www.arxiv.org/pdf/1612.03975.pdf"
                  ],
                  [
                    "accessDate",
                    "2018-04-09 15:52:16"
                  ],
                  [
                    "title",
                    "arXiv:1612.03975 PDF"
                  ]
                ],
                "resource": "storage/876FFAXX/Speer_et_al._-_2016_-_ConceptNet_5.5_An_Open_Multilingual_Graph_of_Gene.pdf"
              },
              {
                "text": "arXiv.org Snapshot",
                "item-id": "i285",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "arXiv.org Snapshot",
                "item_type": "attachment",
                "metadata": [
                  [
                    "url",
                    "https://arxiv.org/abs/1612.03975"
                  ],
                  [
                    "accessDate",
                    "2018-04-09 15:52:17"
                  ],
                  [
                    "title",
                    "arXiv.org Snapshot"
                  ]
                ],
                "resource": "storage/WBMSKFWA/1612.html"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "ConceptNet 5.5",
            "item_type": "journalArticle",
            "metadata": [
              [
                "url",
                "http://arxiv.org/abs/1612.03975"
              ],
              [
                "publicationTitle",
                "arXiv:1612.03975 [cs]"
              ],
              [
                "date",
                "2016-12-12 2016-12-12"
              ],
              [
                "extra",
                "arXiv: 1612.03975"
              ],
              [
                "accessDate",
                "2018-04-09 15:52:15"
              ],
              [
                "libraryCatalog",
                "arXiv.org"
              ],
              [
                "abstractNote",
                "Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies."
              ],
              [
                "title",
                "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"
              ],
              [
                "shortTitle",
                "ConceptNet 5.5"
              ]
            ],
            "selectable": false
          }
        ],
        "item_title": "Word Vectors",
        "item_type": null,
        "selectable": false
      },
      {
        "text": "Semantic Role Labeler",
        "item-id": "c70,i524",
        "nodes": [
          {
            "text": "Deep Semantic Role Labeling: What Works and What\u2019s Next",
            "item-id": "i524",
            "nodes": [
              {
                "text": "He et al. - Deep Semantic Role Labeling What Works and What\u2019s.pdf",
                "item-id": "i523",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "He et al. - Deep Semantic Role Labeling What Works and What\u2019s.pdf",
                "item_type": "attachment",
                "metadata": [
                  [
                    "url",
                    "https://homes.cs.washington.edu/%7Eluheng/files/acl2017_hllz.pdf"
                  ],
                  [
                    "accessDate",
                    "2018-07-11 21:51:57"
                  ],
                  [
                    "title",
                    "He et al. - Deep Semantic Role Labeling What Works and What\u2019s.pdf"
                  ]
                ],
                "resource": "storage/ZV8D4R9L/He_et_al._-_Deep_Semantic_Role_Labeling_What_Works_and_What\u2019s.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Deep Semantic Role Labeling: What Works and What\u2019s Next",
            "item_type": "journalArticle",
            "metadata": [
              [
                "pages",
                "11"
              ],
              [
                "libraryCatalog",
                "Zotero"
              ],
              [
                "language",
                "en"
              ],
              [
                "abstractNote",
                "We introduce a new deep learning model for semantic role labeling (SRL) that signi\ufb01cantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations. We use a deep highway BiLSTM architecture with constrained decoding, while observing a number of recent best practices for initialization and regularization. Our 8-layer ensemble model achieves 83.2 F1 on the CoNLL 2005 test set and 83.4 F1 on CoNLL 2012, roughly a 10% relative error reduction over the previous state of the art. Extensive empirical analysis of these gains show that (1) deep models excel at recovering long-distance dependencies but can still make surprisingly obvious errors, and (2) that there is still room for syntactic parsers to improve these results."
              ],
              [
                "title",
                "Deep Semantic Role Labeling: What Works and What\u2019s Next"
              ]
            ],
            "selectable": false
          }
        ],
        "item_title": "Semantic Role Labeler",
        "item_type": null,
        "selectable": false
      },
      {
        "text": "TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing",
        "item-id": "i383",
        "nodes": [
          {
            "text": "Carreras et al. - 2008 - TAG, dynamic programming, and the perceptron for e.pdf",
            "item-id": "i382",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Carreras et al. - 2008 - TAG, dynamic programming, and the perceptron for e.pdf",
            "item_type": "attachment",
            "metadata": [
              [
                "url",
                "http://delivery.acm.org/10.1145/1600000/1596327/p9-carreras.pdf?ip=128.248.4.97&id=1596327&acc=OPEN&key=B63ACEF81C6334F5%2EAACB7351D18CAF98%2E4D4702B0C3E38B35%2E6D218144511F3437&__acm__=1525099128_63d9b67159c39d8cb1008da33123e52a"
              ],
              [
                "accessDate",
                "2018-04-30 14:33:50"
              ],
              [
                "title",
                "Carreras et al. - 2008 - TAG, dynamic programming, and the perceptron for e.pdf"
              ]
            ],
            "resource": "storage/U7WHMECA/Carreras_et_al._-_2008_-_TAG,_dynamic_programming,_and_the_perceptron_for_e.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-pencil",
        "item_title": "TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing",
        "item_type": "conferencePaper",
        "metadata": [
          [
            "url",
            "http://portal.acm.org/citation.cfm?doid=1596324.1596327"
          ],
          [
            "place",
            "Manchester, United Kingdom"
          ],
          [
            "publisher",
            "Association for Computational Linguistics"
          ],
          [
            "pages",
            "9-16"
          ],
          [
            "ISBN",
            "978-1-905593-48-4"
          ],
          [
            "date",
            "2008-00-00 2008"
          ],
          [
            "DOI",
            "10.3115/1596324.1596327"
          ],
          [
            "accessDate",
            "2018-04-30 14:33:51"
          ],
          [
            "libraryCatalog",
            "Crossref"
          ],
          [
            "language",
            "en"
          ],
          [
            "abstractNote",
            "We describe a parsing approach that makes use of the perceptron algorithm, in conjunction with dynamic programming methods, to recover full constituent-based parse trees. The formalism allows a rich set of parse-tree features, including PCFGbased features, bigram and trigram dependency features, and surface features. A severe challenge in applying such an approach to full syntactic parsing is the ef\ufb01ciency of the parsing algorithms involved. We show that ef\ufb01cient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism. A lower-order dependency parsing model is used to restrict the search space of the full model, thereby making it ef\ufb01cient. Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy."
          ],
          [
            "title",
            "TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing"
          ],
          [
            "proceedingsTitle",
            "Proceedings of the Twelfth Conference on Computational Natural Language Learning"
          ]
        ],
        "selectable": false
      }
    ],
    "item_title": "NLP",
    "item_type": null,
    "selectable": false
  },
  {
    "text": "Machine Learning",
    "item-id": "c37,i288",
    "nodes": [
      {
        "text": "Causal Learning",
        "item-id": "c62,i478",
        "nodes": [
          {
            "text": "Why Propensity Scores Should Not Be Used for Matching",
            "item-id": "i478",
            "nodes": [
              {
                "text": "King and Nielsen - Why Propensity Scores Should Not Be Used for Match.pdf",
                "item-id": "i476",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "King and Nielsen - Why Propensity Scores Should Not Be Used for Match.pdf",
                "item_type": "attachment",
                "metadata": [
                  [
                    "title",
                    "King and Nielsen - Why Propensity Scores Should Not Be Used for Match.pdf"
                  ]
                ],
                "resource": "storage/3AQH844D/King_and_Nielsen_-_Why_Propensity_Scores_Should_Not_Be_Used_for_Match.pdf"
              },
              {
                "text": "Video: https://www.youtube.com/watch?v=rBv39pK1iEs",
                "item-id": "n488",
                "icon": "glyphicon glyphicon-text-background",
                "item_title": "Video: https://www.youtube.com/watch?v=rBv39pK1iEs",
                "item_type": "note"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "Why Propensity Scores Should Not Be Used for Matching",
            "item_type": "journalArticle",
            "metadata": [
              [
                "pages",
                "33"
              ],
              [
                "libraryCatalog",
                "Zotero"
              ],
              [
                "language",
                "en"
              ],
              [
                "abstractNote",
                "We show that propensity score matching (PSM), an enormously popular method of preprocessing data for causal inference, often accomplishes the opposite of its intended goal \u2014 thus increasing imbalance, inef\ufb01ciency, model dependence, and bias. PSM supposedly makes it easier to \ufb01nd matches by projecting a large number of covariates to a scalar propensity score and applying a single model to produce an unbiased estimate. However, in observational analysis the data generation process is rarely known and so users typically try many models before choosing one to present. The weakness of PSM comes from its attempts to approximate a completely randomized experiment, rather than, as with other matching methods, a more ef\ufb01cient fully blocked randomized experiment. PSM is thus uniquely blind to the often large portion of imbalance that can be eliminated by approximating full blocking with other matching methods. Moreover, in data balanced enough to approximate complete randomization, either to begin with or after pruning some observations, PSM approximates random matching which, we show, increases imbalance even relative to the original data. Although these results suggest researchers replace PSM with one of the other available matching methods, propensity scores have many other productive uses."
              ],
              [
                "title",
                "Why Propensity Scores Should Not Be Used for Matching"
              ]
            ],
            "selectable": false
          }
        ],
        "item_title": "Causal Learning",
        "item_type": null,
        "selectable": false
      },
      {
        "text": "Notes on CG and LM-BFGS optimization of logistic regression",
        "item-id": "i288",
        "nodes": [
          {
            "text": "Citeseer - Full Text PDF",
            "item-id": "i293",
            "icon": "glyphicon glyphicon-paperclip",
            "item_title": "Citeseer - Full Text PDF",
            "item_type": "attachment",
            "metadata": [
              [
                "url",
                "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.134.8575&rep=rep1&type=pdf"
              ],
              [
                "accessDate",
                "2018-04-09 17:01:30"
              ],
              [
                "title",
                "Citeseer - Full Text PDF"
              ]
            ],
            "resource": "storage/47HBUYVN/Iii_-_Notes_on_CG_and_LM-BFGS_Optimization_of_Logistic_R.pdf"
          }
        ],
        "icon": "glyphicon glyphicon-file",
        "item_title": "Notes on CG and LM-BFGS optimization of logistic regression",
        "item_type": "journalArticle",
        "metadata": [
          [
            "volume",
            "198"
          ],
          [
            "pages",
            "282"
          ],
          [
            "publicationTitle",
            "Paper available at http://pub. hal3. name# daume04cg-bfgs, implementation available at http://hal3. name/megam"
          ],
          [
            "date",
            "2004-00-00 2004"
          ],
          [
            "title",
            "Notes on CG and LM-BFGS optimization of logistic regression"
          ]
        ],
        "selectable": false
      }
    ],
    "item_title": "Machine Learning",
    "item_type": null,
    "selectable": false
  },
  {
    "text": "Submissions",
    "item-id": "c17,iNone",
    "nodes": [
      {
        "text": "Task",
        "item-id": "c18,i511",
        "nodes": [
          {
            "text": "A Supervised Approach To The Interpretation Of Imperative To-Do Lists",
            "item-id": "i511",
            "nodes": [
              {
                "text": "Comment: 9 pages; https://github.com/plandes/todo-task/",
                "item-id": "n512",
                "icon": "glyphicon glyphicon-text-background",
                "item_title": "Comment: 9 pages; https://github.com/plandes/todo-task/",
                "item_type": "note"
              },
              {
                "text": "arXiv.org Snapshot",
                "item-id": "i514",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "arXiv.org Snapshot",
                "item_type": "attachment",
                "metadata": [
                  [
                    "url",
                    "https://arxiv.org/abs/1806.07999"
                  ],
                  [
                    "accessDate",
                    "2018-06-25 01:33:25"
                  ],
                  [
                    "title",
                    "arXiv.org Snapshot"
                  ]
                ],
                "resource": "storage/9XHBU9AM/1806.html"
              },
              {
                "text": "Landes - A Supervised Approach To The Interpretation Of Imp.pdf",
                "item-id": "i516",
                "icon": "glyphicon glyphicon-paperclip",
                "item_title": "Landes - A Supervised Approach To The Interpretation Of Imp.pdf",
                "item_type": "attachment",
                "metadata": [
                  [
                    "title",
                    "Landes - A Supervised Approach To The Interpretation Of Imp.pdf"
                  ]
                ],
                "resource": "storage/WI4KGABJ/Landes_-_A_Supervised_Approach_To_The_Interpretation_Of_Imp.pdf"
              }
            ],
            "icon": "glyphicon glyphicon-file",
            "item_title": "A Supervised Approach To The Interpretation Of Imperative To-Do Lists",
            "item_type": "journalArticle",
            "metadata": [
              [
                "url",
                "http://arxiv.org/abs/1806.07999"
              ],
              [
                "publicationTitle",
                "arXiv:1806.07999 [cs]"
              ],
              [
                "date",
                "2018-06-20 2018-06-20"
              ],
              [
                "extra",
                "arXiv: 1806.07999"
              ],
              [
                "accessDate",
                "2018-06-25 01:33:23"
              ],
              [
                "libraryCatalog",
                "arXiv.org"
              ],
              [
                "abstractNote",
                "To-do lists are a popular medium for personal information management. As to-do tasks are increasingly tracked in electronic form with mobile and desktop organizers, so does the potential for software support for the corresponding tasks by means of intelligent agents. While there has been work in the area of personal assistants for to-do tasks, no work has focused on classifying user intention and information extraction as we do. We show that our methods perform well across two corpora that span sub-domains, one of which we released."
              ],
              [
                "title",
                "A Supervised Approach To The Interpretation Of Imperative To-Do Lists"
              ]
            ],
            "selectable": false
          }
        ],
        "item_title": "Task",
        "item_type": null,
        "selectable": false
      }
    ],
    "item_title": "Submissions",
    "item_type": null,
    "selectable": false
  }
]