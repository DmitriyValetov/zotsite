Notes on CG and LM-BFGS Optimization of Logistic Regression

Hal Daum´e III Information Sciences Institute 4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292 hdaume@isi.edu

1 Introduction
It has been recognized that the typical iterative scaling methods [?, ?] used to train logistic regression classiﬁcation models (maximum entropy models) are quite slow. Goodman has suggested the use of a component-wise optimization of GIS [?], which he has measured to be faster on many tasks. However, in general, the iterative scaling methods pale in comparison to conjugate gradient ascent (for binary problems) and limited memory BFGS for multiclass problems [?, ?, ?]. Unfortunately, while these methods are typically algorithmically more eﬃcient than the iterative scaling algorithms, they are also signiﬁcantly more diﬃcult to implement (especially LMBFGS). This paper describes one particular implementation that is known to be quite fast, and has gone through several iterations of optimization. The actual implementation can be downloaded from http://www.isi.edu/~hdaume/megam/ and may be used freely for any research purposes1. The intended audience of this paper is quite technical: I assume you know what logistic regression/maximum entropy models are, I assume you know what gradients and Hessians are, etc.

2 Notation

We will assume throughout that we have N many training iid data points, xn and

their corresponding classes yn. Each data point will have F many components, denoted xnf . For binary problems, we assume yn ∈ {−1, +1} and for multiclass

problems, we assume yn ∈ {1, 2, . . . , C} where C is the total number of classes. We

will use a Gaussian prior on weights with precision (inverse variance) λ, and will

denote our weight vector w of length F . Dot products will be written using matrix

notation, eg w w will be the 2-norm of w; correspondingly, ww will denote an

F × F matrix. We will typically denote gradients by g, a vector of length F , and

Hessians by H, a square matrix with dimension F . In general, subscripts will the

the lower-case version of their upper bound, and vectors will be indexed from 1 (i.e.,

F f =1

xnf

or

N n=1

yn

).

In

such

cases,

we

will

typically

leave

oﬀ

the

upper

and

lower

bounds from the sum or product to simplify notation. In general, if there is a vector

1Suitable acknowledgment is appreciated, either in the form of a footnote or a reference to this paper (a bibtex entry can be found on the web page); If you wish to use the software for commercial/non-research purposes, please contact me.

http://pub.hal3.name#daume04cg-bfgs

v that changes over iterations, v will refer to the value at the current iteration, and v will refer to the value at the previous iteration (though our algorithms will explicitly update these).

3 Conjugate Gradient Ascent
The basic idea in CG is to select our search direction so that it is perpendicular to the search direction from the previous iteration; see [?] for further details. In particular, if u is an arbitrary direction, we update w by:

w ←w+ λu u +

gu n σ (w xn) σ (−w xn) (u xn)2 u

and σ is the logistic function, σ(a) = (1 + exp −a)−1; the gradient is given by:

g = −λw + σ −ynw xn ynxn
n
We choose u according to u ← g − βu, where a good value of β is according to the Hestenes-Stiefel formula:

β=g u

(g − g) (g − g)

As can be observed, the value w xn appears quite frequently in all expressions. Implementationally, it is very important to cache this value and update it between iterations, rather than constantly recompute it. My recommended implementation is shown in Figure 1. In general with the problems we work with, each xn will be sparse, and will typically be implemented by storing two arrays, one for indices and one for feature values (note that the indices need not be sorted). In the algorithm, we need to compute both dense dot products, and dot products between sparse vectors xn and dense vectors. Both of these can be implemented eﬃciently, the ﬁrst in a direct sum/loop, the latter in a loop over the indices of xn, summing the corresponding components of the dense vector. Note that in the computation of β in the algorithm, we do not need to actually construct a new vector g − g, but can rather take the dot product implicitly. Finally, it is recommended that the memory for g be allocated outside the loop, so we do not waste time doing so inside.

4 Limited Memory BFGS
For multiclass problems, it becomes impossible to explicitly construct and invert the Hessian matrix. In the binary instance, the Hessian had a simple form the enabled simple analytic inversion; the alternative used in LM-BFGS is to use only an approximation to the true Hessian, and to build this approximation up iteratively. In particular, we will approximate the Hessian at iteration i using the previous M values of the weight vector and of the gradient (of course, when i < M , we only use the i − 1 most recent).
In order to facilitate the use of such memories, we introduce a new data structure that contains three arrays of length exactly M (doubly linked lists would also work), one for the old weight vectors, one for the old gradient vectors, and one for a scalar.

Algorithm CG(x, y, λ) Initialize w ← 0 F , wtx ← 0 N , g ← 0 F , u ← 0 F while not converged do
g ← −λw for n = 1 . . . N do
g ← g + σ (−yn wtx[n]) ynxn end for β ← g (g − g) / u (g − g) u ← g − βu z ← g u / λu u + n σ ( wtx[n]) σ (− wtx[n]) (u xn)2 w ← w + zu for n = 1 . . . N do
wtx[n] ← wtx[n] + zu xn end for g←g end while return w
Figure 1: The full training algorithm for conjugate gradient ascent.
The data structure should support a push operation that adds a new vector pair and scalar to the memory, overwriting the oldest if necessary. We also need to be able to iterate through the memory both from most recent back, and from least recent forward. This is implemented in the data structure memory in the implementation referenced in the introduction.
In LM-BFGS, we take steps according to:
w ← w − ηHg
where η is a step size parameter. For η = 1, this is a Newton step; however, step sizes < 1 are useful and so we use a line search algorithm [?] to ﬁnd it (this is described later). The LM-BFGS trick is to be able to compute Hg in a reasonable amount of time, using our memory. This trick is described in general terms in [?, ?] and will simply be used in our algorithm, specialized to the case of logistic regression.
As in CG, it is important to cache the dot product of the weights with the feature vectors. However, in this case, storing such values takes a matrix of size N ×C, since we need to store it for each2 class. Nevertheless, doing so is quite imperative to eﬃcient optimization; otherwise, all execution time is spent on function evaluation. Additionally, we also compute a value qtx which stores the dot product of the change in weight by x, also of size N × C. The algorithm for LM-BFGS optimization is depicted in Figure 5. This requires three subroutines, ComputeGradient, ComputePosterior and LineSearch. These are depicted in Figures 2, 3 and 4, respectively.
The main LM-BFGS algorithm essentially performs one one iteration without using any Hessian information (the part before the while loop) and then begins the loop, using previous iteration’s gradients. At each iteration it ﬁnds a step parameter η by calling the LineSearch algorithm. In our experience, this algorithm terminates after zero, one or two iterations, and its computations are very inexpensive, so there is no need to use a more complex line search. The notation memd[m] means the mth vector d to be pushed into memory, where m = M means the most recent

Algorithm ComputeGradient(x, y, w, wtx) g ← −λw for n = 1, . . . , N do
z ← c wtx[n, c] for c = 1, . . . , C do
g ← g + (δc,yn − exp(wtx[n, c] − z)) xnc end for end for return g
Figure 2: The ComputeGradient subroutine required for LM-BFGS.
Algorithm ComputePosterior(λ, y, w, q, wtx, qtx, η) p ← −λ/2 w w + η2q q + 2ηq w for n = 1, . . . , N do
s ← −∞ for c = 1, . . . , C do
χ ← wtx[n, c] + ηqtx[n, c] s←s⊕χ if c = yn then
p ← p + wtx[n, c] end if end for p←p−s end for return p
Figure 3: The ComputePosterior subroutine required for LineSearch.
and m = 1 means the least recent; the other subscripts are the same. Again, it is advantageous to allocate all memory outside any loops and compute using only existing arrays. In fact, we can gain some memory savings by storing the vectors d and u in the approximate Hessian computation in place of g, and then restoring it later (see my implementation for this slight trick).
The computation of the gradient and posterior make use of the ⊕ operator, which is deﬁned to be addition of values in log-space. This can be implemented eﬃciently and each ⊕ operation requires a logarithm and exponentiation computation. is simply a -sum using ⊕ instead of +.
In the ComputePosterior algorithm, within one line search, the values w w, q q and q w will always be the same, so it is recommended that these are cached outside of the ComputePosterior and passed in as arguments (this is done in my implementation as well).
Finally, the line search is a simple backtracking line search [?]. This uses the technique of modeling the (negative log) posterior by a cubic and explicitly maximizing it each time. Note that the maximization need not converge – we only need a value of η that attains suﬃcient decrease. In the full LM-BFGS algorithm, if the value η returned is ever zero, then the iterations need to stop (if this is not done, then on the next iteration things will blow up).

5 Summary
I have described eﬃcient implementations of the conjugate gradient and limited memory BFGS methods for optimizing logistic regression classiﬁers. I have made available a public implementation of these methods to demonstrate their eﬀectiveness on real world problems. The algorithms described herein are completely selfcontained and require no digging through literature to ﬁnd sub-components. This was done, perhaps, at a slight loss in generality, but the reader is directed to [?, ?] for more general details. It is my sincere hope that these notes and the implementation are helpful to some users.
References

Algorithm LineSearch(λ, y, wtx, w, q, g, qtx)

τ ← min{1, 100/ q q} fold ←ComputePosterior(λ, y, q, wtx, qtx, 0) γ ← τg q
if γ < 0 then
return 0

end if
ηmin ← 1e-10/ maxf ((τ abs qf )/ max{(abs wf ), 1}) η ← 1, ηold ← 0, f2 ← fold while true do
if τ η < ηmin then return 0

else

f ←ComputePosterior(λ, y, q, wtx, qtx, ητ )

if f ≥ fold + 1e-4τ ηγ then

return τ η

else if abs(η − 1) < 1e-20 then

ηtmp ← γ/(2(fold + γ − f )) ηold ← η, η ← max{ηtmp, (η/10)}, f2 ← f

else

r1 ← f − fold − γη

r2 ← f2 − fold − γηold

a b

← ←

(r1/η2 − (ηr2/ηo2ld

r−2/ηηoo2ldldr)1//(ηη2−)/(ηηol−d)

ηold

ηtmp ← 0

if abs a < 1e-20 then

ηtmp ← −γ/(2b)

else

d ← b2 − 3aγ

if d < 0 then

ηtmp ← η/2

else if b <=√0 then ηtmp ← ( d − b)/(3a)

else

√

ηtmp ← −γ/(b + d)

end if

end if
ηtmp ← min{ηtmp, η/2} ηold ← η, η ← max{ηtmp, (η/10)}, f2 ← f end if

end if

end while return τ η

Figure 4: The LineSearch subroutine required for LM-BFGS.

Algorithm LM-BFGS(x, y, λ) Initialize w ← 0 F , wtx ← 0 N×C g ←ComputeGradient(x, y, w, wtx)
q ← g/ g g
qtx ← q x η ←LineSearch(λ, y, wtx, w, q, g, qtx) for n = 1 . . . N, c = 1 . . . C do
wtx[n, c] ← wtx[n, c] + η qtx[n, c] xnc end for w ← w + ηq Initialize mem ← ∅ while not converged do
g ←ComputeGradient(x, y, w, wtx) α ← (g − g) (w − w) σ ← (g − g) (g − g) Push d = (w − w), u = (g − g) and α onto mem q←g β← 0M for m = M, . . . , 1 do
β[m] ← (memd[m]) / (memα[m]) q ← q − β[m] (memu[m]) end for q ← σq for m = 1, . . . , M do ζ ← (memu[m]) q for f = 1, . . . , F do
ξ ← (memd[m, f ]) (β[m] − ζ/ (mema[m])) q[f ] ← q[f ] + ξ ζ ←ζ+ξ end for
end for q ← −q qtx ← q x η ←LineSearch(λ, y, wtx, w, q, g, qtx) for n = 1 . . . N, c = 1 . . . C do
wtx[n, c] ← wtx[n, c] + η qtx[n, c] xnc end for w ← w + ηq g←g end while
return w
Figure 5: The full training algorithm for limited memory BFGS.

