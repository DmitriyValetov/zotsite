VOL. 70, No. 4

OCTOBER 1968

Psychological Bulletin

WEIGHTED KAPPA:

NOMINAL SCALE AGREEMENT WITH PROVISION FOR SCALED DISAGREEMENT OR PARTIAL CREDIT
JACOB COHEN1
New York University

A previously described coefficient of agreement for nominal scales, kappa, treats all disagreements equally. A generalization to weighted kappa (KW) is presented. The KW provides for the incorporation of ratio-scaled degrees of disagreement (or agreement) to each of the cells of the k X k table of joint nominal scale assignments
such that disagreements of varying gravity (or agreements of varying degree) are weighted accordingly. Although providing for partial credit, KW is fully chance corrected. Its sampling characteristics and procedures for hypothesis testing and setting confidence limits are given. Under certain conditions, KW equals productmoment r. Although developed originally as a measure of reliability, the use of unequal weights for symmetrical cells makes KW suitable as a measure of validity.

A previous article (Cohen, 1960) described K (kappa), an index of agreement for use with nominal scales, as analogous to an alternate form reliability coefficient for magnitudescaled data. Reliability has long been the keystone of psychometric theory (Gulliksen, 1950; Rozeboom, 1966), but the basic models have been developed for one-dimensional equal in-
terval scales. Reliability plays the same crucial role in nominal scales as it does in magnitude scales (e.g., setting an upper bound for empirical validity), yet the relevant methodological literature is impoverished. The need to assess nominal scale reliability arises in fields as diverse as psychiatric diagnosis (Spitzer, Cohen, Fleiss, & Endicott, 1967) and survey interview coding (Scott, 1955).
Past approaches to the problem were deficient both in the indices used to measure degree of agreement and in their statistical treatment. The most frequently used index has
1 The author is greatly indebted to Joseph L. Fleiss of Columbia University School of Public Health for developing the asymptotic standard error of an observed K«I. Acknowledgmentsare also due Robert L. Spitzer of Biometrics Research, whose research in computerbased psychiatric diagnosis stimulated the work reported here, and Patricia Waly, for a critical reading of the manuscript.

been percentage or proportion of agreement (po—in Table 1), which suffers in that it includes agreement which can be accounted for by chance. Occasionally, the k X k table of joint categorical assignment frequencies (where each "judge" has made assignments to the same &-level nominal scale) has been treated as a contingency table, and the contingency coefficient, C, based on chi-square, X2, (McNemar, 1962) has been used as a measure of agreement. The defect of X2 in this context, and therefore of C, is that it indexes association and not necessarily agreement, which is the special kind of association of interest in reliability. In an agreement matrix, high reliability dictates that the values observed in the k cells of the leading or agreement diagonal be higher than the chance expectation dictated by the marginal values, and that, conversely, the off-diagonal cells representing disagreement have observed values which are smaller than those expected by chance. The X2 and hence C increase monotonically with increases in the absolute discrepancies between observed and chance-expected values in each of the cells, whether these discrepancies are in the direction of agreement or disagreement, quite impartially.

213

© 1968 by the American Psychological Association, Inc.

214

JACOB COHEN

Cohen (1960) proposed as a coefficient of used for setting confidence limits and perform-

agreement for nominal scales, the proportion ing two-sample hypothesis tests, and the

of agreement corrected for chance

standard error of Kwhen the population K = 0

Po — PC

[1]

1 - PC

where p0 is the observed proportion of agreement, and pc is the proportion of agreement expected by chance. This is found by summing over the agreement diagonals, the product of the proportions for the row and column of the cell, as illustrated by the parenthetical values in each cell of Table 1. Cohen also presented large sample formulae for the standard error of an observed K

OK — N(\ -

[2]

= -y

[3]

used for one-sample significance tests of K (1960, Formulas 7 and 10). For large samples, the sampling distribution of K is approximately normal, and statistical tests using normal curve deviates take the familiar classical form
(Cohen, 1960). Thus, K provides a conceptually simple mea-
sure of reliability for nominal scales: the proportion of agreement after agreement which can be attributed to chance has been removed both from the base and from the numerator, as

TABLE 1 AN AGREEMENT MATRIX or PROPORTIONS WITH ILLUSTRATIVE COMPUTATIONS
OP K, KW AND RELEVANT STATISTICS
Judge A

Diagnostic Category

ij

Personality disorder

Neurosis

Personality disorder

G» (.30)b .44"

1 (.18) .07

Judge B

Neurosis

1 (.15) .05

0 (.09) .20

Psychosis

3 (.05) .01

6 (.03) .03

P.i

.50

.30

Note.—N = 200

«« = 1 - #„ = 1 - (.44 + .20 + .06) = .30 5, = 1 - p, = 1 - (.30 + .09 + .02) = .59

Psychosis
3 (.12) .09
6 (.06) .05
0 (.02) .06
.20

I, = OU4) + 1(.07) + 3 (.09) + 1 (.05) + • • • + 0(.06) = .90

0«(.44)

+ 3*(.09) -f

3.90 - .90' TOO(1.382)

.0901

+ 0!(.06) = 3.UD + 0«(.02) = 5.10

a h

Disagreement weight Chance-expected eel!

»« proportion,

' Observed cell proportion, pan

/S.10 - 1.38' ""-o = V 200(1.38lT = ' 916
l>5% Confidence Limits on KU,: KW rt 1.96o-K .348 ± 1.96(.0<J01) =

Pi. .60 .30 .10 1.00
[4]
[8]
[10] [13]

WEIGHTED KAPPA

215

Formula 1 directly sets forth. It quite reasonably yields negative values when there is less observed agreement than is expected by chance, zero when observed agreement can be (exactly) accounted for by chance, and unity when there is complete agreement.
The further development to ««, (weighted kappa) is motivated by studies in which it is the sense of the investigator that some disagreements in assignments, that is, some offdiagonal cells in the k X k matrix, are of greater gravity than others. For example, in an assessment of the reliability of psychiatric diagnosis in the categories: (a) personality disorder (D), (b) neurosis (N), and (c) psychosis (P), a clinician would likely consider a diagnostic disagreement between neurosis and psychosis to be more serious than between neurosis and personality disorder (see Table 1). The K makes no such distinction, implicitly treating all disagreement cells equally. This article describes the development of KW, the proportion of weighted agreement corrected for chance, to be used when different kinds of disagreement are to be differentially weighted in the agreement index.
DEVELOPMENT OF WEIGHTED KAPPA
The desired weighting is accomplished by an a priori assignment of weights to the W cells of the k X k matrix, that is, a ratio scaling of the cells. Either degree of agreement or degree of disagreement may be scaled, depending on what seems more natural in the given context. The development here will be in terms of disagreement ratio scaling, for example, 6 represents twice as much disagreement as 3. This will be supplemented later with formulae for use with agreement scaling. Note that in either case, the result is KW, a chance-corrected proportion of weighted agreement.
We begin with the basic formula for K (Equation 1). If we define q = 1 —p as the proportion of disagreement, p = 1 — q. Substituting po = 1 — <?„ and pc = 1 — qc into Equation 1 and simplifying yields

replaces q0 and qc by proportions of weighted disagreement, q'0 and q'c. To find the latter, each of the &2 cells must have a disagreement weight, vu, where the ij subscript indexes the cell (i,j = !•••&). These (positive) weights can be assigned by means of any judgment procedure set up to yield a ratio scale (Torgersen, 1958) including the simple direct scaling advocated by Stevens (1958). In many instances, they may be the result of a consensus of a committee of substantive experts, or even, conceivably, the investigator's own judgment. They are, in any case, to be ratio weights. It is convenient (but not necessary) to assign zero to the "perfect" agreement diagonal (i = j ) , that is, no disagreement. A weight which represents maximum disagreement (vmax) is assigned at the convenience of the investigator (for Table 1, it is 6). For any set of va, Kw is invariant over any positive multiplicative transformation, that is, KTO will not change if its iiy are multiplied by any value greater than zero.
The brief attention to the setting of these weights should not mislead the reader as to their importance. The weights assigned are an integral part of how agreement is defined and therefore how it is measured with KW. Moreover, its standard error is also a function of the Vij (or for agreement weighting, the w«), so that the results of significance tests are also dependent upon the weights. Another way of stating this is that the weights are part of any hypothesis being investigated. An obvious consequence of this is that the weights, however determined, must be set prior to the collection of the data.
Proportions of weighted disagreement, observed and chance, are simply weighted functions over the &2 cells of the p0ij and />0y, respectively, namely

/ 2-f ViiPoij

p.-,

<? o — „,

L5J

=1

[4]

an equation for K expressed in terms of observed and chance disagreement. KW simply

where the /><,,•,• is the proportion of the joint judgments (N in number) observed in the ij cell, and the pdj the proportion in the cell expected by chance, as illustrated in Table 1.

216

JACOB COHEN

(The summation throughout is over all cells.) Weighted kappa is then given by

[7] qc
When Formulas 5 and 6 are substituted in Formula 7, the vmax term drops out, and it simplifies to

Kw

_

l1 _

2-JW'«J
v" „..* ..

r-Q-i
L°J

Table 1 illustrates the computation of both K and KW, using unweighted and weighted disagreement proportions, respectively. The matrix of proportions of joint assignments in Table 1 is obtained from the usual k X k table of joint frequencies or paired assignments in which cell, marginal and total (N) observed frequencies have been divided by the latter. To find the pca, the marginal proportions for the ij cell are multiplied, for example, for the upper left cell, pcii = pi.p.j = pi.p.i = (.60) (.50) = .30, given in parentheses in that cell.
For K, only the pafj and pdj values in the agreement diagonal (i = j ) are needed, and K is found from Formula 4 to equal .492, that is, after chance agreement is excluded, about half the judgments are in agreement, all disagreements being counted equally.
For KTO, the weighted sums over all cells (numerators of Formulas 5 and 6) are substituted in Formula 8.2 For the vtj in Table 1, K» = .348.
The values of Table 1 were selected in order to emphasize a point which might otherwise go unappreciated: like K, KW is fully chance corrected. One might suppose, since the cells are
scaled for degrees of disagreement, that this is like not giving some cells full disagreement credit (i.e., the obverse of giving partial agreement credit), and that therefore KW relative to K is biased in an upward direction, that is, it overstates agreement. The premise is correct, but the consequent is not. The same weights which generate q'0 also generate q'c', and KW may well be smaller than K for the same data, as is the case in Table 1. This will occur
2 When the VH in the diagonal cells are set at zero, they contribute to neither Equation 5 nor 6, so that in practice, the summations are actually over only k2 — k cells.

when the algebraically smaller values of
pdj — poij occur in cells which have large Upvalues. It occurs in Table 1 because the N - P(2,3) and P - N(3,2) disagreement cells, which have the largest »,-; = 6 show peij — pan discrepancies of only .01 and .00, while the less serious D — N and N — D disagreement cells (iiij = 1) show discrepancies of .12 and .10. This means that these judges disagree much less than chance expectation where it doesn't count very much and disagree at about the chance level where it counts greatly. The result is KW smaller than K. If the Vij's of 6 and 1 are interchanged in the table, KW becomes .574, a value greater than K.
For a computing formula using frequencies rather than proportions, one simply substitutes / for p values in Formula 8

Ky, = 1 —

[9]

where /„,-; is the observed frequency in cell ij, and fcn is the chance-expected frequency in cell ij, computed as for a X2 contingency table.

Sampling Characteristics
The asymptotic (large sample approximation) standard error of KW is

or, in terms of cell frequencies

The use of Formula 10 is illustrated in
Table 1. (Note that it requires, in addition to the terms required by Formula 8, the determination of ]T if-ijpoij.) Since the sampling distribution of KW is approximately normal for large samples, KW can be used for setting confidence limits on a sample KM together with the appropriate unit normal curve deviate (illustrated in Table 1 for 95% limits, where z = ± 1.96), and also for a normal curve test of the significance of the difference between two independent KWS

z = --l"1-^-"*'-

,. / • )

10

[12] i_ _i

WEIGHTED KAPPA

217

To test an obtained KW for significance, the standard error of KW when the population KW equals zero is needed. It is obtained by substituting chance for observed cell proportions where the latter appear in Formula 10

nri LJ

the computation of which is illustrated in

Table 1. In terms offrequencies

VCTKu

[14]

A significance test of KW, that is, a test of Ho: Population KW = 0, is accomplished by evaluating the normal curve deviate

to the diagonal (i = /) cells representing complete agreement (full credit).3 The use of zero as the minimum «>,-/ ("no" credit) is convenient, but not necessary. As with the »,•,-, KW is invariant over multiplication of the wy by any value greater than zero. The stress on the importance of the »y when they were discussed in the preceding section extend, of course, to the wy.
Parallel to the above development, we define weighted proportions of observed and chance agreement

E *>tit

[16]

[17]
W

[15]
For the data in Table 1, KW = ,348, = .0901, and
.348 z = .0916 = 3.80, significant at p < .001.
It should be noted that the demonstration that a population KW is greater than zero, at whatever significance level, is, in general, hardly impressive. The KW here is a reliability coefficient, and one normally wishes evidence that the population KW is some relatively large value, rather than merely that it highly probably exceeds zero. Thus, in most instances, a substantial value for the lower confidence limit (at, say, 95%) rather than zero as implied by the null hypothesis, is a more meaningful criterion for the adequancy of nominal scale reliability.

By replacing weighted for unweighted proportions of agreement in the basic formula for K (Formula 1), we obtain

P'o - P'c 1-p'c

[18]

Substituting the values of Equations 16 and 17 and simplifying yields

_ E Wijpoij - E Wijpcij

KW —

^i

Wmax — 2-

n(n
L-'-'J

In terms of frequencies

E Wiifoi, - E

[20]

—E

Also

^=V; oH - (E

[21]

and, in terms of frequencies

Weighted Kappa through Agreement Scaling
The KW can be developed with cell weights which reflect agreement (wy) rather than disagreement (tin). When the concept of "full" credit for complete agreement and varying amounts of "partial" credit (possibly including no credit) for different off-diagonal (i ^ f) cells seems natural in a given context, agreement is scaled so as to yield a ratio scale of positive agreement weights, wy, ranging down from some convenient maximum value assigned

^ N(wmaxN -

[22]

Finally, <TKMO is given, for proportions and

frequencies, respectively, by replacing ob-

served by chance values wherever the former

appear in Formulas 21 and 22.

3 Because of the ratio property of both the v/y and vn, the relationship between the two kinds of weights is complementary when they are expressed as proportions of their respective maximum values. Specifically, KW remains constant when (wa/wmax) = 1 — (vy/Vmaz)
Which yields •VH, = (iVnuut/Vmax) (Vmax — *v).

218

JACOB COHEN

Statistical manipulations, such as setting confidence limits and performing the significance tests of Formulas 12 and 15 are, of course, performed in exactly the same way as when their components were found through disagreement weights.
WEIGHTED KAPPA AND KAPPA
A perspective on KW is afforded by considering its relationship to K. The K is simply proportion of agreement (/>„) corrected for chance, and KW can readily be thought of as a generalization of K, proportion of weighted agreement (as in Formula 16) corrected for chance. The relationship may be more clearly understood if it is inverted: K is a special case of KW. In KW we may differentially weight, either by »<;- or Wij, the off-diagonal (i ^ j) cells, because we mean to consider the various kinds of disagreement as representing differing amounts of disagreement (or, equivalently, differing amounts of agreement). For K, the k (k — 1) off-diagonal cells representing disagreement are simply treated as if they all represented the same amount of disagreement. It is a matter of simple algebra to show that if all vy(i ^ j) are given the same weight, unity or any other value greater than the agreement diagonal weights (= 0 in the example), K«, becomes K. That is, with »,-,• (i — j) = 0 and all »y (i ^ j) = a constant, the constant cancels out in Formula 8 for KW, leaving K of Formula 4. Similarly, a constant value for w,v (i -^ j), smaller than another constant WH (i = j) = Wmax, reduces Formula 19 for KV to Formula 1 for K. Thus, K is the special case of KW where all disagreements are given the same weight. Furthermore, under these conditions, the standard error formulas for KW simplify to those for K (Formulas 2 and 3).
WEIGHTED KAPPA AND PRODUCTMOMENT CORRELATION
It is a frequent experience for the methodologist exploring an area apparently remote from product-moment correlation (r) to turn a corner and find it confronting him (for a recent example see Glass, 1966). A discovery of this kind may be of greater importance in its illumination of r than of the area being explored. Such a discovery was made in the case of KW.

Under certain simple conditions, KW = r. The conditions are these: (a) The marginal distributions are the same, that is,pi. = p,jfor i = j. (b) Disagreement weights (»#) are assigned according to the following pattern: The k cells of the agreement diagonal (i = J) have tty = 0. The k — 1 cells in each of the two adjacent diagonals have »,-y of 1, the k — 2 cells in each of the next diagonals out on either side have vfj = 22 = 4, the k — 3 cells in each of the next diagonals have »,•/ = 32 = 9, and so on until one reaches the last cell in the upper right and lower left corner whose weights are (k — I)2. For example, for k = 5, the pattern of vn is
0 1 4 9 16 10149 41014 9410 1 16 9 4 1 0
Using these weights, one can compute KW with Formulas 8 or 9.
Now, give the nominal categories scores equal to their index numbers, that is, the first category is scored 1, the second is scored 2, etc. If the product-moment r is computed from the observed frequencies or proportions using these scores (or linear transformations thereof), r is found identical to the KW above.4
This identity did not come as a complete surprise. In the article presenting K (Cohen, 1960), it was shown that for the 2 X 2 table under the equal marginal condition, K — <j>, the fourfold point correlation coefficient (phi coefficient). For the 2 X 2 table with symmetrical assignment of weights (»«= vz\, or wiz = W«I)KW perforce equals K. On the other hand, <> is simply a product-moment r for di-
chotomous data. Thus, the previous finding of K = <j> is a special case of the present more general finding that KW = r under the stated conditions.
4 The proof proceeds by writing the "difference" formula for r, then letting the means and standard deviations of the two distributions be equal (as in Cohen, 1957, Formula 5), the latter being a consequence of the first condition. In this form, the formula for r has the same structure as that of KW (Formulas 8 and 9). If one then notes that the »y of the required pattern are, in fact, equal to /V = (x< ~ Yi^ for V = M' • •*, one can see how the proof proceeds.

WEIGHTED KAPPA

219

Note that the two conditions, equal marginals and the prescribed pattern of weights, are not of equal importance. Relaxation of the first to the degree of inequality of marginals normally found in real data reduces the equality of KW and r to a close approximation, with KW < r, but by no more than a few hundredths.
The equality of «„ and r under the stated conditions is primarily of interest for the new conception it offers for r. It means that r can be conceived as a (suitably chance-corrected) proportion of agreement, where the disagreements are weighted so as to be proportional to the square of the distance between the pair of measures, or, equivalently, from the X = Y line. (In the general case, where X and 7 are not expressed in the same units, they must be conceived as first being transformed to a common standard score.) Perhaps this should not be surprising, given the role of least squares in the definition of r. The tendency of statistical neophytes to interpret r as a proportion may be more constructively dealt with pedagogically by showing them by this route (there are others) exactly what kind of a proportion it is.
WEIGHTED KAPPA AS A VALIDITY MEASURE
The examples and discussion to this point have implicitly assigned equal weights to symmetric cells, that is, tiy = «y,- (or WH = Wji). An error which comes about from Judge A assigning "Neurotic" where Judge B assigns "Psychotic" is of the same gravity, or gets the same partial credit as one in which their assignments are reversed. This is appropriate to the frame of reference of reliability, where the two sources of data are conceived as being of equal status, that is, as alternate forms. Some reflection suggests that the formal difference between reliability and validity lies in the contrast between the equal status of the sources in the former and their differing status in validity, where one is a predictor and the other a criterion. When validity is being assessed, it may (but need not) be eminently reasonable for vy 7* 11n (or wu 7* Wji). It is this conception which is operative in the different costs attached to false positives and false negatives in
dichotomous diagnosis, and in the different

values given producer's risk and consumer's risk in statistical quality control. Since there is nothing in the conception or statistical manipulation of KW which demands weight symmetry, it can be used for k X k tables constructed for assessing nominal (and indeed, stronger than merely nominal) scale validity.
For example, reinterpret the situation in Table 1 as follows: Consider Judge A to be the consensus diagnosis of a panel of distinguished diagnosticians—the criterion; and Judge B the diagnosis made by a computer—the predictor, or variable being tested (Spitzer et al., 1967). Given the way the computer diagnosis is to be used, it may well be considered, for example, that a computer error in making a diagnosis of Neurosis when the panel consensus is Psychosis is more serious than a computer diagnosis of Psychosis when the panel consensus is Neurosis. This is realized in the definition of agreement by assigning different weights to these symmetrical cells. For this use of KW, the pattern of »y which are finally assigned may look like this:

D Computer N
P

Panel DNP
0 14 106 2 20

Such a pattern implies a greater concern about failing to identify psychotics (more so by calling them neurotics) than for mistakenly identifying them, and less (and symmetrical) concern for the Neurosis-Personality Disorder
confusion, whichever way the error is made. Assuming the proportions of Table 1 with
these new weights, it is found that ^ vypoa — -86, JZ Vijpdi= 1-33, and therefore KW = .353 (Formula 8). With these new weights, fftn = .0887 (Formula 10) and craw, = .0915 (Formula 13). The KV remains the chance-corrected proportion of weighted agreement, but now the weights reflect the "costs" or "utilities" perceived in this situation and
their structure is appropriate to what is intended by the "validity" of computer diagnosis.

REFERENCES
COHEN, J. An aid in the computation of correlations based on Q sorts. Psychological Bulletin, 1957, 54, 138-139.

220

JACOB COHEN

COHEN, J. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 1960, 20, 37-46.
GLASS, G. V. Note on rank biserial correlation. Educational and Psychological Measurement, 1966, 26, 623631.
GULLIKSEN, H. Theory of mental tests. New York:Wiley, 1950.
McNEMAR, Q. Psychological statistics. (3rd ed.) NewYork: Wiley, 1962.
ROZEBOOM, W. VV. Foundations of the theory of prediction. Homewood, 111.: Dorscy, 1966.

SCOTT, W. A. Reliability of content analysis: The case of nominal scale coding. Public Opinion Quarterly, 1955, 19, 321-325.
SPITZER, R. L., COHEN, J., FLEISS, J. L., & ENDICOTT, J. Quantification of agreement in psychiatric diagnosis. A new approach. Archives of General Psychiatry, 1967, 17, 83-87.
STEVENS, S. S. Problems and methods of psychophysics. Psychological Bulletin, 1958, 55, 177-196.
TORGERSEN, W. S. Theory and methods of scaling. New York: Wiley, 1958.
(Received October 19, 1967)

